{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/seby/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/seby/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y must have at least two dimensions for multi-output regression but has only one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 66\u001b[0m\n\u001b[1;32m     60\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     61\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m, vectorizer),\n\u001b[1;32m     62\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, model)\n\u001b[1;32m     63\u001b[0m ])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Predict the skills for the test set\u001b[39;00m\n\u001b[1;32m     69\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(y_test)\n",
      "File \u001b[0;32m~/Desktop/School/2024_Spring/cs4120/group_project/JobAssist/myenv/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/School/2024_Spring/cs4120/group_project/JobAssist/myenv/lib/python3.12/site-packages/sklearn/pipeline.py:475\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    474\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 475\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/School/2024_Spring/cs4120/group_project/JobAssist/myenv/lib/python3.12/site-packages/sklearn/multioutput.py:537\u001b[0m, in \u001b[0;36mMultiOutputClassifier.fit\u001b[0;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[1;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and targets Y.\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;124;03m        Returns a fitted instance.\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 537\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m [estimator\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;28;01mfor\u001b[39;00m estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_]\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/School/2024_Spring/cs4120/group_project/JobAssist/myenv/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/School/2024_Spring/cs4120/group_project/JobAssist/myenv/lib/python3.12/site-packages/sklearn/multioutput.py:246\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    243\u001b[0m     check_classification_targets(y)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my must have at least two dimensions for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti-output regression but has only one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m     )\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: y must have at least two dimensions for multi-output regression but has only one."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('job_descriptions.csv')  # Replace 'job_descriptions.csv' with the actual file path\n",
    "df = df.head(20000)\n",
    "\n",
    "# Preprocess the data\n",
    "df['skills'] = df['skills'].apply(lambda x: x.split(','))\n",
    "\n",
    "# Function to remove punctuation and stopwords\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply text preprocessing to the Job Description column\n",
    "df['Job Description'] = df['Job Description'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Job Description'], df['skills'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the CountVectorizer for text feature extraction / tokenization\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Initialize the MultiLabelBinarizer for encoding the skills\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Transform the skills sets using MultiLabelBinarizer\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_test = mlb.transform(y_test)\n",
    "\n",
    "# Define the LogisticRegression model wrapped in MultiOutputClassifier for multi-label classification\n",
    "model = MultiOutputClassifier(LogisticRegression(solver='lbfgs'))\n",
    "\n",
    "# Create a pipeline that first vectorizes the text and then applies the classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, X_test)\n",
    "\n",
    "# Predict the skills for the test set\n",
    "y_pred = pipeline.predict(y_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: We are looking for a front-end software engineer with experience in Python, Java, and C++, leadership, and marketing.\n",
      "Predicted Skills: [()]\n"
     ]
    }
   ],
   "source": [
    "# Example of predicting skills for a new job description\n",
    "new_job_description = preprocess_text(df[\"Job Description\"].iloc[0])\n",
    "new_job_description = \"We are looking for a front-end software engineer with experience in Python, Java, and C++, leadership, and marketing.\"\n",
    "\n",
    "#  expected_skills = df[\"skills\"].iloc[0]\n",
    "print(f\"Job Description: {new_job_description}\")\n",
    "# print(f\"Expected skills: {expected_skills}\")\n",
    "predicted_skills = pipeline.predict([new_job_description])\n",
    "print(f'Predicted Skills: {mlb.inverse_transform(predicted_skills)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seby/Desktop/School/2024_Spring/cs4120/group_project/JobAssist/myenv/lib/python3.12/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"An ETL Developer specializes in designing and impl...\" with entities \"[(67, 77, 'SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/Users/seby/Desktop/School/2024_Spring/cs4120/group_project/JobAssist/myenv/lib/python3.12/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"As a Cloud Systems Engineer, you will be responsib...\" with entities \"[(166, 170, 'SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/Users/seby/Desktop/School/2024_Spring/cs4120/group_project/JobAssist/myenv/lib/python3.12/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"A Data Analyst Researcher conducts research and an...\" with entities \"[(14, 16, 'SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 90.60213024474773}\n",
      "{'ner': 2.1948040794394566e-06}\n",
      "{'ner': 2.3259453398928506e-08}\n",
      "{'ner': 6.63459987229098e-07}\n",
      "{'ner': 7.02590685384092e-08}\n",
      "{'ner': 7.20473847071517e-09}\n",
      "{'ner': 6.127857370880504e-10}\n",
      "{'ner': 1.51065689790898e-06}\n",
      "{'ner': 6.353318021424173e-08}\n",
      "{'ner': 1.4642113375719021e-09}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "import random\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('job_descriptions.csv').head(20000)\n",
    "\n",
    "# Preprocess the dataset\n",
    "# Convert the Skills column into a list\n",
    "df['skills'] = df['skills'].apply(lambda x: x.split(','))\n",
    "\n",
    "# Function to preprocess the data into Spacy's required format\n",
    "def preprocess_data(df):\n",
    "    data = []\n",
    "    for index, row in df.iterrows():\n",
    "        entities = []\n",
    "        for skill in row['skills']:\n",
    "            if skill in row['Job Description']:\n",
    "                start = row['Job Description'].index(skill)\n",
    "                end = start + len(skill)\n",
    "                entities.append((start, end, 'SKILL'))\n",
    "        if entities:\n",
    "            data.append((row['Job Description'], {'entities': entities}))\n",
    "    return data\n",
    "\n",
    "# Convert the dataset\n",
    "TRAIN_DATA = preprocess_data(df)\n",
    "\n",
    "# Load a blank Spacy model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Create a new NER pipeline\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe('ner', last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "\n",
    "# Add the label 'SKILL' to the NER\n",
    "ner.add_label('SKILL')\n",
    "\n",
    "# Disable other pipelines during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "# Training the NER model\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(10):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        for text, annotations in TRAIN_DATA:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            nlp.update([example], drop=0.5, sgd=optimizer, losses=losses)\n",
    "        print(losses)\n",
    "\n",
    "# Save the trained model\n",
    "nlp.to_disk('ner_skill_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: Social Media Managers oversee an organizations social media presence. They create and schedule content, engage with followers, and analyze social media metrics to drive brand awareness and engagement.\n",
      "Expected skills: ['Social media platforms (e.g.', ' Facebook', ' Twitter', ' Instagram) Content creation and scheduling Social media analytics and insights Community engagement Paid social advertising']\n",
      "Detected skills:\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "nlp = spacy.load('ner_skill_model')\n",
    "\n",
    "# New job description\n",
    "example = df[\"Job Description\"].iloc[0]\n",
    "expected_skills = df[\"skills\"].iloc[0]\n",
    "doc = nlp(example)\n",
    "print(f\"Job Description: {example}\")\n",
    "print(f\"Expected skills: {expected_skills}\")\n",
    "\n",
    "# Print detected skills\n",
    "print(\"Detected skills:\")\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'SKILL':\n",
    "        print(ent.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
